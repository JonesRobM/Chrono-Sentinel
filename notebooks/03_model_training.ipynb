{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates training the transformer model and evaluating with MC Dropout uncertainty.\n",
    "\n",
    "## Contents\n",
    "1. Model architecture overview\n",
    "2. Training the model\n",
    "3. MC Dropout uncertainty quantification\n",
    "4. Evaluation and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from threatsim.data import get_dataloaders\n",
    "from threatsim.models import create_model, mc_dropout_predict\n",
    "from threatsim.utils import set_seed, get_device\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "train_loader, val_loader, test_loader, class_weight = get_dataloaders(\n",
    "    window_size=50, batch_size=32\n",
    ")\n",
    "print(f\"Train batches: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")\n",
    "print(f\"Class weight: {class_weight.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Architecture\n",
    "\n",
    "Our transformer architecture:\n",
    "- Linear projection to model dimension (64)\n",
    "- Sinusoidal positional encoding\n",
    "- 2 transformer encoder layers (4 attention heads)\n",
    "- Mean pooling + classification head\n",
    "- Dropout throughout for MC Dropout at inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(window_size=50, d_model=64, num_layers=2, dropout=0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"\\nArchitecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = torch.nn.BCELoss(reduction='none')\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "def weighted_loss(predictions, labels):\n",
    "    loss = criterion(predictions, labels)\n",
    "    weights = torch.where(labels == 1, class_weight.to(device), torch.ones_like(labels))\n",
    "    return (loss * weights).mean()\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for windows, labels in train_loader:\n",
    "        windows, labels = windows.to(device), labels.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        preds = model(windows)\n",
    "        loss = weighted_loss(preds, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        epoch_loss += loss.item()\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for windows, labels in val_loader:\n",
    "            windows, labels = windows.to(device), labels.to(device)\n",
    "            preds = model(windows)\n",
    "            val_loss += weighted_loss(preds, labels).item()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, 'b-', label='Training')\n",
    "ax.plot(val_losses, 'r-', label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MC Dropout Uncertainty Quantification\n",
    "\n",
    "Monte Carlo Dropout: keep dropout active at inference time and run multiple forward passes. The variance across passes gives us uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions with uncertainty\n",
    "all_labels = []\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "for windows, labels in test_loader:\n",
    "    windows = windows.to(device)\n",
    "    mean, std = mc_dropout_predict(model, windows, n_samples=30)\n",
    "    all_labels.append(labels.numpy())\n",
    "    all_means.append(mean.cpu().numpy())\n",
    "    all_stds.append(std.cpu().numpy())\n",
    "\n",
    "test_labels = np.concatenate(all_labels)\n",
    "test_preds = np.concatenate(all_means)\n",
    "test_uncertainty = np.concatenate(all_stds)\n",
    "\n",
    "print(f\"Test samples: {len(test_labels)}\")\n",
    "print(f\"Mean uncertainty: {test_uncertainty.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "binary_preds = (test_preds >= 0.5).astype(int)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, binary_preds, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "if len(np.unique(test_labels)) > 1:\n",
    "    print(f\"ROC-AUC: {roc_auc_score(test_labels, test_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty analysis\n",
    "correct = binary_preds == test_labels\n",
    "\n",
    "print(f\"\\nUncertainty Analysis:\")\n",
    "print(f\"  Average uncertainty (correct predictions): {test_uncertainty[correct].mean():.4f}\")\n",
    "print(f\"  Average uncertainty (incorrect predictions): {test_uncertainty[~correct].mean():.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(test_uncertainty[correct], bins=30, alpha=0.6, label=f'Correct (n={correct.sum()})', color='green')\n",
    "ax.hist(test_uncertainty[~correct], bins=30, alpha=0.6, label=f'Incorrect (n={(~correct).sum()})', color='red')\n",
    "ax.set_xlabel('Uncertainty (Std Dev)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Uncertainty Distribution: Correct vs Incorrect')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with uncertainty bands\n",
    "n_show = min(200, len(test_labels))\n",
    "x = np.arange(n_show)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 6), sharex=True)\n",
    "\n",
    "ax1.fill_between(x, \n",
    "    np.clip(test_preds[:n_show] - 2*test_uncertainty[:n_show], 0, 1),\n",
    "    np.clip(test_preds[:n_show] + 2*test_uncertainty[:n_show], 0, 1),\n",
    "    alpha=0.3, color='blue', label='95% CI')\n",
    "ax1.plot(x, test_preds[:n_show], 'b-', linewidth=1, label='Prediction')\n",
    "ax1.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title('Predictions with MC Dropout Uncertainty')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.fill_between(x, 0, test_labels[:n_show], alpha=0.5, color='red')\n",
    "ax2.set_xlabel('Sample')\n",
    "ax2.set_ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have:\n",
    "1. Trained a transformer for anomaly detection\n",
    "2. Applied MC Dropout for uncertainty quantification\n",
    "3. Demonstrated that the model is more uncertain when making incorrect predictions\n",
    "\n",
    "For a complete training run with more epochs and proper checkpointing, use:\n",
    "```bash\n",
    "python scripts/train.py\n",
    "python scripts/evaluate.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
